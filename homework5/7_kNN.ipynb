{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiAO-t29WvsV"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1tElT3FhddabqCyJKFH27rhnyXJTqrzke?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA6TVm_IWvsY"
      },
      "source": [
        "# K Nearest Neighbors (KNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPUvUP5bWvsZ"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNuZl141WvsZ"
      },
      "source": [
        "The k Nearest Neighbors method (kNN) is a very popular classification method, also sometimes used in regression tasks. It's one of the most understandable approaches to classification. Intuitively, the essence of the method is: look at the neighbors; the predominant ones indicate what you are. Formally, the basis of the method is the compactness hypothesis: if the distance metric between examples is introduced successfully, then similar examples are much more likely to be in the same class than in different ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXLgjkTIWvsa"
      },
      "source": [
        "<img src='https://hsto.org/web/68d/a45/6f0/68da456f00f8434e87628dbe7e3f54a7.png' width=600>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-ApkYISWvsa"
      },
      "source": [
        " For the classification of each object in the test dataset, it is necessary to sequentially perform the following operations:\n",
        "   * Calculate the distance to each of the objects in the training dataset.\n",
        "   * Select objects from the training dataset to which the distance is minimal.\n",
        "   * The class of the classified object is the class most frequently occurring among the k nearest neighbors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPw6bq-TWvsa"
      },
      "source": [
        "We will work with a subset of the [Covertype dataset from the UCI repository](http://archive.ics.uci.edu/ml/datasets/Covertype). There are 7 different classes available. Each object is described by 54 features, 40 of which are binary. The data description is available at the link."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w0g3lTMWvsb"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vg7ASD0yWvsb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jH3hlePQWvsc"
      },
      "source": [
        "Press [here](https://drive.google.com/file/d/1Z39LNnF4lOj4iT48YDYFZNPDBglMd0TZ/view?usp=sharing) to download the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzWEujiYWvsc"
      },
      "outputs": [],
      "source": [
        "all_data = pd.read_csv('datasets/forest_dataset.csv')\n",
        "all_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZbMypXqWvsc"
      },
      "outputs": [],
      "source": [
        "all_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb45pIcyWvsd"
      },
      "source": [
        "Extract the class labels into a variable `labels`, and the feature descriptions into a variable `feature_matrix`. Since the data is numerical and does not have missing values, we will convert it to `numpy` format using the `.values` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htTgmopmWvsd"
      },
      "outputs": [],
      "source": [
        "labels = all_data[all_data.columns[-1]].values\n",
        "feature_matrix = all_data[all_data.columns[:-1]].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVivKd0LWvsd"
      },
      "source": [
        "## Train-test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4I-N-3UBWvsd"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kmQjAGiWvsd"
      },
      "source": [
        "We will now work with all 7 types of cover (the data is already in the variables `feature_matrix` and `labels` if you haven't redefined them). Divide the sample into training and test sets using the `train_test_split` method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7zxr7yKWvsd"
      },
      "outputs": [],
      "source": [
        "train_feature_matrix, test_feature_matrix, train_labels, test_labels = train_test_split(\n",
        "    feature_matrix, labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARAl6YePWvsd"
      },
      "source": [
        "The main objects in `sklearn` are the so-called `estimators`. They are divided into **classifiers** and **regressors**.\n",
        "\n",
        "As examples of models, you can consider the classifiers\n",
        "[k Nearest Neighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) and\n",
        "[logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl2_xbY8Wvsd"
      },
      "source": [
        "All models in `sklearn` must have at least 2 methods  -- `fit` and `predict`.\n",
        "The `fit(X, y)` method is responsible for training the model and takes as input a training sample in the form of a *feature matrix* $X$ and a *response vector* $y$.\n",
        "\n",
        "After `fit`, you can now call the `predict(X)` method on the trained model, which will return predictions of this model on all objects from the matrix $X$ in the form of a vector.\n",
        "\n",
        "You can call `fit` on the same model several times; each time it will be trained anew on the given dataset.\n",
        "\n",
        "Also, models have *hyperparameters*, which are usually set when creating the model.\n",
        "\n",
        "Let's see all this on the example of logistic regression.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTHwxiIuWvsd"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unLV_r1dWvsd"
      },
      "outputs": [],
      "source": [
        "clf = LogisticRegression(C=1)\n",
        "clf.fit(train_feature_matrix, train_labels)\n",
        "y_pred = clf.predict(test_feature_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pq0TdzzIWvsd"
      },
      "source": [
        "Now, we would like to measure the quality of our model. For this, we can use the `score(X, y)` method, which will calculate some error function on the sample $X, y$, but exactly which one depends on the model. Alternatively, one of the functions from the `metrics` module can be used, for example, [accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html), which, as the name suggests, will calculate the accuracy of predictions for us.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-HxGtJlWvsd"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy_score(test_labels, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzTCAELEWvse"
      },
      "source": [
        "Finally, the last thing we would like to mention is grid search for hyperparameter tuning. Since models have many hyperparameters that can be changed, and the quality of the model significantly depends on these hyperparameters, it would be desirable to find the best parameters in this sense. The simplest way to do this is to just iterate over all possible variants within reasonable limits.\n",
        "\n",
        "This can be done using the [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) class, which conducts a search (search) across a grid (grid) and calculates the quality of the model using cross-validation (CV).\n",
        "\n",
        "For example, in logistic regression, you can change the `C` and `penalty` parameters. Let's do that. Keep in mind that the search may take a long time. Refer to the documentation for the meaning of the parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0cITtvqWvse"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEs0xOQ_Wvse"
      },
      "outputs": [],
      "source": [
        "clf = LogisticRegression(solver='saga')\n",
        "\n",
        "# init GridSearchCV with parameters\n",
        "param_grid = {\n",
        "    'C': np.arange(1, 5),\n",
        "    'penalty': ['l1', 'l2'],\n",
        "}\n",
        "\n",
        "search = GridSearchCV(clf, param_grid, n_jobs=-1, cv=5, refit=True, scoring='accuracy')\n",
        "\n",
        "search.fit(feature_matrix, labels)\n",
        "\n",
        "print(search.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNpyGzTPWvse"
      },
      "source": [
        "In this case, the search iterates over all possible pairs of values for `C` and `penalty` from the specified sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YvzbS2TWvse"
      },
      "outputs": [],
      "source": [
        "accuracy_score(labels, search.best_estimator_.predict(feature_matrix))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1PzAVSdWvse"
      },
      "source": [
        "Note that we pass the entire dataset to GridSearchCV, not just its training part. This can be done because the search still uses cross-validation. However, sometimes a *validation* part is still separated from the dataset, as the hyperparameters might overfit to the sample during the search process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RIXb3KDWvse"
      },
      "source": [
        "In the tasks, you will need to repeat this for the k Nearest Neighbors method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TACJ1yUXWvse"
      },
      "source": [
        "# Homework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_qRVit4Wvse"
      },
      "source": [
        "The quality of classification/regression by the k Nearest Neighbors method depends on several parameters:\n",
        "* the number of neighbors `n_neighbors`\n",
        "* the distance metric between objects `metric`\n",
        "* the weights of neighbors (the neighbors of the test example can enter with different weights, for example, the further the example, the less its \"voice\" is taken into account) `weights`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwRpi06AWvse"
      },
      "source": [
        "Train the `KNeighborsClassifier` on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7q9evZJWvse"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "clf = # Your code here\n",
        "# ...\n",
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEDiN8x5Wvse"
      },
      "source": [
        "Let's select the parameters of our model:\n",
        "\n",
        "* Iterate over the grid from `1` to `10` for the number of neighbors parameter\n",
        "* Also, try using different metrics: `['manhattan', 'euclidean']`\n",
        "* Try using different weight calculation strategies: `[‘uniform’, ‘distance’]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0zDrGXvWvse"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "params = ...\n",
        "\n",
        "clf_grid = GridSearchCV(clf, params, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "# Train the classifier\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWmutX1xWvse"
      },
      "source": [
        "Let's output the best parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1KHfIhGWvse"
      },
      "outputs": [],
      "source": [
        "clf_grid.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kmRKYzrWvsh"
      },
      "source": [
        "Using the optimal number of neighbors found, calculate the probabilities of belonging to classes for the test sample (`.predict_proba`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wj_NJMBrWvsh"
      },
      "outputs": [],
      "source": [
        "optimal_clf = # ...\n",
        "# Training ...\n",
        "pred_prob = # ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5NxiBRSWvsh"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "\n",
        "unique, freq = np.unique(test_labels, return_counts=True)\n",
        "freq = list(map(lambda x: x / len(test_labels),freq))\n",
        "\n",
        "pred_freq = pred_prob.mean(axis=0)\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.bar(range(1, 8), pred_freq, width=0.4, align=\"edge\", label='prediction')\n",
        "plt.bar(range(1, 8), freq, width=-0.4, align=\"edge\", label='real')\n",
        "plt.ylim(0, 0.54)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZ_gv_RHWvsh"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-n0ExCZbWvsh"
      },
      "source": [
        "What is the quality of the model on the test sample? Compare it with the quality of logistic regression. Which model is better? Why? What are the pros and cons of the k Nearest Neighbors method? Plot the ROC curve for the k Nearest Neighbors method. Calculate the area under the ROC curve (AUC-ROC)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}